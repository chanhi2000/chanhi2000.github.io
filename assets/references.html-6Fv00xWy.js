import{_ as f}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as m,d as r,g as t,b as a,at as s,t as g,w as o,e as l,r as p,o as b}from"./app-DM0WOK1z.js";const c={},k={id:"frontmatter-title-관련",tabindex:"-1"},y={class:"header-anchor",href:"#frontmatter-title-관련"},A={class:"table-of-contents"},L={id:"google",tabindex:"-1"},E={class:"header-anchor",href:"#google"},C={id:"x",tabindex:"-1"},B={class:"header-anchor",href:"#x"},v={id:"medium",tabindex:"-1"},M={class:"header-anchor",href:"#medium"},D={id:"velog",tabindex:"-1"},x={class:"header-anchor",href:"#velog"},w={href:"https://colab.research.google.com/gist/virattt/7b67c685ca6b256d4fa6108bfae53d7a/exploring-llm-pricing-cost.ipynb",target:"_blank",rel:"noopener noreferrer"},_={href:"https://colab.research.google.com/github/rapidsai-community/showcase/blob/main/getting_started_tutorials/cudf_pandas_stocks_demo.ipynb",target:"_blank",rel:"noopener noreferrer"},I={href:"https://gist.github.com/jart/bd2f603aefe6ac8004e6b709223881c0",target:"_blank",rel:"noopener noreferrer"};function j(d,e){const h=p("VPIcon"),n=p("router-link"),i=p("PDF"),u=p("TagLinks");return b(),m("div",null,[r("h1",k,[r("a",y,[r("span",null,g(d.$frontmatter.title)+" 관련",1)])]),r("nav",A,[r("ul",null,[r("li",null,[a(n,{to:"#google"},{default:o(()=>[a(h,{icon:"fa-brands fa-google"}),e[0]||(e[0]=l("Google"))]),_:1}),r("ul",null,[r("li",null,[a(n,{to:"#colab"},{default:o(()=>e[1]||(e[1]=[l("Colab")])),_:1})])])]),r("li",null,[a(n,{to:"#pdf-s"},{default:o(()=>e[2]||(e[2]=[l("pdf(s)")])),_:1})]),r("li",null,[a(n,{to:"#x"},{default:o(()=>[a(h,{icon:"fa-brands fa-x-twitter"}),e[3]||(e[3]=l("X"))]),_:1})]),r("li",null,[a(n,{to:"#nvidia"},{default:o(()=>e[4]||(e[4]=[l("NVIDIA")])),_:1})]),r("li",null,[a(n,{to:"#medium"},{default:o(()=>[a(h,{icon:"fa-brands fa-medium"}),e[5]||(e[5]=l("Medium"))]),_:1})]),r("li",null,[a(n,{to:"#velog"},{default:o(()=>[a(h,{icon:"iconfont icon-velog"}),e[6]||(e[6]=l("velog"))]),_:1})]),r("li",null,[a(n,{to:"#substack"},{default:o(()=>e[7]||(e[7]=[l("Substack")])),_:1})]),r("li",null,[a(n,{to:"#brunch"},{default:o(()=>e[8]||(e[8]=[l("Brunch")])),_:1})]),r("li",null,[a(n,{to:"#inblog"},{default:o(()=>e[9]||(e[9]=[l("inblog")])),_:1})]),r("li",null,[a(n,{to:"#replicate"},{default:o(()=>e[10]||(e[10]=[l("Replicate")])),_:1})]),r("li",null,[a(n,{to:"#metaai"},{default:o(()=>e[11]||(e[11]=[l("MetaAI")])),_:1})]),r("li",null,[a(n,{to:"#google-1"},{default:o(()=>e[12]||(e[12]=[l("Google")])),_:1}),r("ul",null,[r("li",null,[a(n,{to:"#colab-1"},{default:o(()=>e[13]||(e[13]=[l("Colab")])),_:1})])])]),r("li",null,[a(n,{to:"#ollama"},{default:o(()=>e[14]||(e[14]=[l("Ollama")])),_:1})]),r("li",null,[a(n,{to:"#linkedin"},{default:o(()=>e[15]||(e[15]=[l("Linkedin")])),_:1})]),r("li",null,[a(n,{to:"#finbarr-timbers"},{default:o(()=>e[16]||(e[16]=[l("Finbarr Timbers")])),_:1})]),r("li",null,[a(n,{to:"#fast-ai"},{default:o(()=>e[17]||(e[17]=[l("fast.ai")])),_:1})]),r("li",null,[a(n,{to:"#replit"},{default:o(()=>e[18]||(e[18]=[l("replit")])),_:1})]),r("li",null,[a(n,{to:"#second-state"},{default:o(()=>e[19]||(e[19]=[l("Second State")])),_:1})]),r("li",null,[a(n,{to:"#bbycroft"},{default:o(()=>e[20]||(e[20]=[l("Bbycroft")])),_:1})]),r("li",null,[a(n,{to:"#justine"},{default:o(()=>e[21]||(e[21]=[l("Justine")])),_:1})]),r("li",null,[a(n,{to:"#moomou"},{default:o(()=>e[22]||(e[22]=[l("moomou")])),_:1})]),r("li",null,[a(n,{to:"#trail-of-bits"},{default:o(()=>e[23]||(e[23]=[l("Trail of Bits")])),_:1})]),r("li",null,[a(n,{to:"#_2mb-codes"},{default:o(()=>e[24]||(e[24]=[l("2MB codes")])),_:1})]),r("li",null,[a(n,{to:"#shyam-s-blog"},{default:o(()=>e[25]||(e[25]=[l("Shyam's Blog")])),_:1})]),r("li",null,[a(n,{to:"#gradient-descent-into-madness"},{default:o(()=>e[26]||(e[26]=[l("Gradient Descent into Madness")])),_:1})]),r("li",null,[a(n,{to:"#hamel"},{default:o(()=>e[27]||(e[27]=[l("Hamel")])),_:1})]),r("li",null,[a(n,{to:"#ahead-of-ai"},{default:o(()=>e[28]||(e[28]=[l("Ahead of AI")])),_:1})]),r("li",null,[a(n,{to:"#kapa-ai"},{default:o(()=>e[29]||(e[29]=[l("kapa.ai")])),_:1})]),r("li",null,[a(n,{to:"#cloudflare"},{default:o(()=>e[30]||(e[30]=[l("Cloudflare")])),_:1})]),r("li",null,[a(n,{to:"#martin-lumiste"},{default:o(()=>e[31]||(e[31]=[l("Martin Lumiste")])),_:1})]),r("li",null,[a(n,{to:"#brandon-s-digital-garden"},{default:o(()=>e[32]||(e[32]=[l("Brandon's Digital Garden")])),_:1})]),r("li",null,[a(n,{to:"#justine-tunney-s-web-page"},{default:o(()=>e[33]||(e[33]=[l("Justine Tunney's Web Page")])),_:1})]),r("li",null,[a(n,{to:"#hiddenest"},{default:o(()=>e[34]||(e[34]=[l("hiddenest")])),_:1})]),r("li",null,[a(n,{to:"#tistory"},{default:o(()=>e[35]||(e[35]=[l("tistory")])),_:1})]),r("li",null,[a(n,{to:"#real-python"},{default:o(()=>e[36]||(e[36]=[l("Real Python")])),_:1})]),r("li",null,[a(n,{to:"#정우일-블로그"},{default:o(()=>e[37]||(e[37]=[l("정우일 블로그")])),_:1})]),r("li",null,[a(n,{to:"#daddy-maker"},{default:o(()=>e[38]||(e[38]=[l("Daddy Maker")])),_:1})]),r("li",null,[a(n,{to:"#outerbounds"},{default:o(()=>e[39]||(e[39]=[l("Outerbounds")])),_:1})]),r("li",null,[a(n,{to:"#christophergs"},{default:o(()=>e[40]||(e[40]=[l("ChristopherGS")])),_:1})]),r("li",null,[a(n,{to:"#simon-willison-s-tils"},{default:o(()=>e[41]||(e[41]=[l("Simon Willison's TILs")])),_:1})]),r("li",null,[a(n,{to:"#allen-pike"},{default:o(()=>e[42]||(e[42]=[l("Allen Pike")])),_:1})]),r("li",null,[a(n,{to:"#냉동코더의-기술블로그"},{default:o(()=>e[43]||(e[43]=[l("냉동코더의 기술블로그")])),_:1})]),r("li",null,[a(n,{to:"#applied-llms"},{default:o(()=>e[44]||(e[44]=[l("Applied LLMs")])),_:1})]),r("li",null,[a(n,{to:"#oranlooney-com"},{default:o(()=>e[45]||(e[45]=[l("OranLooney.com")])),_:1})]),r("li",null,[a(n,{to:"#lytix-ai"},{default:o(()=>e[46]||(e[46]=[l("lytix.ai")])),_:1})]),r("li",null,[a(n,{to:"#llama-ttf"},{default:o(()=>e[47]||(e[47]=[l("llama.ttf")])),_:1})]),r("li",null,[a(n,{to:"#eugene-yan"},{default:o(()=>e[48]||(e[48]=[l("Eugene Yan")])),_:1})]),r("li",null,[a(n,{to:"#alex-strick-van-linschoten"},{default:o(()=>e[49]||(e[49]=[l("Alex Strick van Linschoten")])),_:1})]),r("li",null,[a(n,{to:"#imbue"},{default:o(()=>e[50]||(e[50]=[l("imbue")])),_:1})]),r("li",null,[a(n,{to:"#jay-alammar"},{default:o(()=>e[51]||(e[51]=[l("Jay Alammar")])),_:1})]),r("li",null,[a(n,{to:"#the-missing-notes"},{default:o(()=>e[52]||(e[52]=[l("The Missing Notes")])),_:1})]),r("li",null,[a(n,{to:"#포티투닷-42dot-we-are-a-mobility-ai-company"},{default:o(()=>e[53]||(e[53]=[l("포티투닷 | 42dot - We Are A Mobility AI Company")])),_:1})]),r("li",null,[a(n,{to:"#haandol-tl-dr"},{default:o(()=>e[54]||(e[54]=[l("Haandol, TL;DR")])),_:1})]),r("li",null,[a(n,{to:"#비즈니스-테크놀로지-리더십-cio-korea"},{default:o(()=>e[55]||(e[55]=[l("비즈니스, 테크놀로지, 리더십 - CIO Korea")])),_:1})]),r("li",null,[a(n,{to:"#테크놀로지-리더를-위한-글로벌-it-뉴스-itworld-korea"},{default:o(()=>e[56]||(e[56]=[l("테크놀로지 리더를 위한 글로벌 IT 뉴스 - ITWorld Korea")])),_:1})]),r("li",null,[a(n,{to:"#speaker-deck-easily-share-your-presentations-online"},{default:o(()=>e[57]||(e[57]=[l("Speaker Deck | Easily Share Your Presentations Online")])),_:1})]),r("li",null,[a(n,{to:"#dable-tech-blog"},{default:o(()=>e[58]||(e[58]=[l("Dable Tech Blog")])),_:1})]),r("li",null,[a(n,{to:"#augmend"},{default:o(()=>e[59]||(e[59]=[l("Augmend")])),_:1})]),r("li",null,[a(n,{to:"#min-hsu"},{default:o(()=>e[60]||(e[60]=[l("Min Hsu")])),_:1})]),r("li",null,[a(n,{to:"#chip-huyen"},{default:o(()=>e[61]||(e[61]=[l("Chip Huyen")])),_:1})]),r("li",null,[a(n,{to:"#codesolvent-blog"},{default:o(()=>e[62]||(e[62]=[l("Codesolvent Blog")])),_:1})]),r("li",null,[a(n,{to:"#최윤섭의-디지털-헬스케어"},{default:o(()=>e[63]||(e[63]=[l("최윤섭의 디지털 헬스케어")])),_:1})]),r("li",null,[a(n,{to:"#timescale-blog"},{default:o(()=>e[64]||(e[64]=[l("Timescale Blog")])),_:1})]),r("li",null,[a(n,{to:"#_000namc-blog"},{default:o(()=>e[65]||(e[65]=[l("000namc.blog")])),_:1})]),r("li",null,[a(n,{to:"#sequoia-capital"},{default:o(()=>e[66]||(e[66]=[l("Sequoia Capital")])),_:1})]),r("li",null,[a(n,{to:"#_000namc-xyz"},{default:o(()=>e[67]||(e[67]=[l("000namc.xyz")])),_:1})]),r("li",null,[a(n,{to:"#tensormsa"},{default:o(()=>e[68]||(e[68]=[l("TensorMSA")])),_:1})]),r("li",null,[a(n,{to:"#jason-kang"},{default:o(()=>e[69]||(e[69]=[l("Jason Kang")])),_:1})]),r("li",null,[a(n,{to:"#pega-devlog"},{default:o(()=>e[70]||(e[70]=[l("Pega Devlog")])),_:1})]),r("li",null,[a(n,{to:"#dschloe"},{default:o(()=>e[71]||(e[71]=[l("DSChloe")])),_:1})])])]),e[81]||(e[81]=r("hr",null,null,-1)),r("h2",L,[r("a",E,[r("span",null,[a(h,{icon:"fa-brands fa-google"}),e[72]||(e[72]=l("Google"))])])]),e[82]||(e[82]=r("h3",{id:"colab",tabindex:"-1"},[r("a",{class:"header-anchor",href:"#colab"},[r("span",null,"Colab")])],-1)),e[83]||(e[83]=r("ul",null,[r("li",null,[r("a",{href:"https://colab.research.google.com/drive/1iw3TPRBSXOd5EgXLJbTiN6uFO1mCoq6C",target:"_blank",rel:"noopener noreferrer"},"Knowledge Distillation")]),r("li",null,[r("a",{href:"https://colab.research.google.com/drive/132oXSGSOyzZ7GO9pJhRKlvwY4F-i9Pm6",target:"_blank",rel:"noopener noreferrer"},[r("code",null,"Selecting_an_embedding_model_for_custom_data.ipynb")])])],-1)),t(" END: colab.research.google.com "),e[84]||(e[84]=r("hr",null,null,-1)),e[85]||(e[85]=r("h2",{id:"pdf-s",tabindex:"-1"},[r("a",{class:"header-anchor",href:"#pdf-s"},[r("span",null,"pdf(s)")])],-1)),e[86]||(e[86]=r("ul",null,[r("li",null,[r("a",{href:"https://allenai.org/olmo/olmo-paper.pdf",target:"_blank",rel:"noopener noreferrer"},"OLMo : Accelerating the Science of Language Models")])],-1)),a(i,{url:"https://arxiv.org/pdf/2306.11025"}),a(i,{url:"https://arxiv.org/pdf/2404.07143"}),a(i,{url:"https://arxiv.org/pdf/2404.14219"}),a(i,{url:"https://arxiv.org/pdf/2407.12994"}),a(i,{url:"https://arxiv.org/pdf/2407.21059"}),a(i,{url:"https://arxiv.org/pdf/2408.11039"}),a(i,{url:"https://arxiv.org/pdf/2409.12089"}),a(i,{url:"https://arxiv.org/pdf/1503.02531"}),a(i,{url:"https://arxiv.org/pdf/2407.16833"}),a(i,{url:"https://arxiv.org/pdf/2410.02525"}),a(i,{url:"https://arxiv.org/pdf/2409.14924"}),a(i,{url:"https://arxiv.org/pdf/2402.14207"}),a(i,{url:"https://arxiv.org/pdf/2409.16694"}),a(i,{url:"https://arxiv.org/pdf/2409.16416"}),a(i,{url:"https://arxiv.org/pdf/2409.15173"}),a(i,{url:"https://arxiv.org/pdf/2409.14924"}),a(i,{url:"https://arxiv.org/pdf/2402.10200"}),e[87]||(e[87]=r("hr",null,null,-1)),r("h2",C,[r("a",B,[r("span",null,[a(h,{icon:"fa-brands fa-x-twitter"}),e[73]||(e[73]=l("X"))])])]),e[88]||(e[88]=s('<ul><li><a href="https://x.com/ariG23498/status/1840967205699367116" target="_blank" rel="noopener noreferrer"><code>ariG23498</code> / A simple hack to calculating how much VRAM you would need to run a model.</a></li><li><a href="https://x.com/mervenoyann/status/1841098941900767323" target="_blank" rel="noopener noreferrer"><code>mervenoyann</code> / NVIDIA just dropped a gigantic multimodal model called NVLM 72B 🦖</a></li><li><a href="https://x.com/danielhanchen/status/1838991771948425652" target="_blank" rel="noopener noreferrer"><code>danielhanchen</code> / Llama 3.2 Multimodal benchmarks</a></li><li><a href="https://x.com/ArtificialAnlys/status/1838955372855390618" target="_blank" rel="noopener noreferrer"><code>ArtificialAnlys</code> / OpenAI’s o1-preview is the first model to substantially push the frontier of language model intelligence since the original GPT-4 over 18 months ago</a></li><li><a href="https://x.com/helloiamleonie/status/1838760385224089769" target="_blank" rel="noopener noreferrer"><code>helloiamleonie</code> / Chunking techniques for RAG:</a></li><li><a href="https://x.com/akshay_pachaar/status/1838556536051999135" target="_blank" rel="noopener noreferrer"><code>akshay_pachaar</code> / Multimodal Retrieval Augmented Generation (RAG), clearly explained:</a></li><li><a href="https://x.com/weaviate_io/status/1842203043724431775" target="_blank" rel="noopener noreferrer"><code>weaviate_io</code> / 6 types of vector embeddings for your AI applications</a></li></ul>',1)),t(" END: x.com "),e[89]||(e[89]=r("hr",null,null,-1)),e[90]||(e[90]=r("h2",{id:"nvidia",tabindex:"-1"},[r("a",{class:"header-anchor",href:"#nvidia"},[r("span",null,"NVIDIA")])],-1)),e[91]||(e[91]=r("ul",null,[r("li",null,[r("a",{href:"https://blogs.nvidia.com/blog/nemotron-4-synthetic-data-generation-llm-training/",target:"_blank",rel:"noopener noreferrer"},"NVIDIA Releases Open Synthetic Data Generation Pipeline for Training Large Language Models")])],-1)),e[92]||(e[92]=r("hr",null,null,-1)),r("h2",v,[r("a",M,[r("span",null,[a(h,{icon:"fa-brands fa-medium"}),e[74]||(e[74]=l("Medium"))])])]),e[93]||(e[93]=s('<ul><li><a href="https://blog.llamaindex.ai/introducing-llamacloud-and-llamaparse-af8cedf9006b" target="_blank" rel="noopener noreferrer"><code>llamaindex</code> / Introducing LlamaCloud and LlamaParse</a></li><li><a href="https://berom0227.medium.com/running-ollama-in-smart-connections-db2242aee3ba" target="_blank" rel="noopener noreferrer"><code>berom0227</code> / Running Ollama in Smart Connections</a></li><li><a href="https://medium.com/thedeephub/50-open-source-options-for-running-llms-locally-db1ec6f5a54f" target="_blank" rel="noopener noreferrer"><code>thedeephub</code> / 50+ Open-Source Options for Running LLMs Locally</a></li><li><a href="https://aksh-garg.medium.com/llama-3v-building-an-open-source-gpt-4v-competitor-in-under-500-7dd8f1f6c9ee" target="_blank" rel="noopener noreferrer"><code>aksh-garg</code> / Llama 3-V: Matching GPT4-V with a 100x smaller model and 500 dollars</a></li><li><a href="https://tpbabparn.medium.com/in-house-llm-application-by-spring-ai-ollama-91c48e2d2d38" target="_blank" rel="noopener noreferrer"><code>tpbabparn</code> / In-house LLM-application by Spring AI + Ollama</a></li><li><a href="https://jhk0530.medium.com/lg-exaone-3-0-0e7221db6356?source=rss-cb820693bed5------2" target="_blank" rel="noopener noreferrer"><code>jhk0530</code> / LG의 오픈소스 AI, 엑사원 3.0 사용후기</a></li><li><a href="https://medium.com/datastrato/building-a-universal-data-agent-in-15-minutes-with-llamaindex-and-apache-gravitino-incubating-401ea24a3b39" target="_blank" rel="noopener noreferrer"><code>datastrato</code> / Building A Universal Data Agent in 15 Minutes with LlamaIndex and Apache Gravitino (incubating)</a></li><li><a href="https://medium.com/@sarmadafzalj/visualize-vector-embeddings-in-a-rag-system-89d0c44a3be4" target="_blank" rel="noopener noreferrer"><code>sarmadafzalj</code> / Visualize Vector Embeddings in a RAG System</a></li></ul>',1)),t(" END: medium.com "),e[94]||(e[94]=r("hr",null,null,-1)),r("h2",D,[r("a",x,[r("span",null,[a(h,{icon:"iconfont icon-velog"}),e[75]||(e[75]=l("velog"))])])]),e[95]||(e[95]=s('<ul><li><a href="https://velog.io/@geoffyoon-dev/cloud-LLM-in-data-security-policy" target="_blank" rel="noopener noreferrer"><code>@geoffyoon-dev</code> - 데이터는 못 보내지만 Cloud LLM은 쓰고싶어</a></li><li><a href="https://velog.io/@devstone/NLP-%EB%A9%94%ED%8A%B8%EB%A6%AD-%ED%86%BA%EC%95%84%EB%B3%B4%EA%B8%B0" target="_blank" rel="noopener noreferrer"><code>@devstone</code> - NLP 메트릭 톺아보기</a></li><li><a href="https://velog.io/@kwon0koang/%EC%98%A4%ED%94%88%EC%86%8C%EC%8A%A4-LLM%EC%9C%BC%EB%A1%9C-RAG-%EC%97%90%EC%9D%B4%EC%A0%84%ED%8A%B8-%EB%A7%8C%EB%93%A4%EA%B8%B0" target="_blank" rel="noopener noreferrer"><code>@kwon0koang</code> / 오픈소스 LLM으로 RAG 에이전트 만들기 (랭체인, Ollama, Tool Calling 대체)</a></li><li><a href="https://velog.io/@eunbibi/%EC%9E%90%EC%97%B0%EC%96%B4%EC%B2%98%EB%A6%AC" target="_blank" rel="noopener noreferrer"><code>@eunbibi</code> / 자연어처리</a></li><li><a href="https://velog.io/@euisuk-chung/NLP-Text-Analytics-Intro" target="_blank" rel="noopener noreferrer"><code>@euisuk-chung</code> / [NLP] 1. Introduction to Text Analytics</a></li><li><a href="https://velog.io/@euisuk-chung/%ED%85%8D%EC%8A%A4%ED%8A%B8-%EB%B6%84%EC%84%9D-%EB%8B%A8%EA%B3%84" target="_blank" rel="noopener noreferrer"><code>@euisuk-chung</code> / [NLP] 2. Steps of Text Analytics</a></li><li><a href="https://velog.io/@euisuk-chung/%EA%BF%80%ED%8C%81-%ED%94%84%EB%A1%AC%ED%94%84%ED%8A%B8-%EC%97%94%EC%A7%80%EB%8B%88%EC%96%B4%EB%A7%81-%EA%B0%95%EC%9D%98-%EC%9A%94%EC%95%BD" target="_blank" rel="noopener noreferrer"><code>@euisuk-chung</code> / [꿀팁] 프롬프트 엔지니어링 (강의 요약)</a></li><li><a href="https://velog.io/@euisuk-chung/IT-LLMOps%EC%99%80-RAG" target="_blank" rel="noopener noreferrer"><code>@euisuk-chung</code> / [IT] LLMOps와 RAG</a></li><li><a href="https://velog.io/@euisuk-chung/2025%EB%85%84-AI-%ED%8A%B8%EB%A0%8C%EB%93%9C-LMM-LAM-%EC%98%A8%EB%94%94%EB%B0%94%EC%9D%B4%EC%8A%A4-AI-AI-%EC%97%90%EC%9D%B4%EC%A0%84%ED%8A%B8-%EC%9E%84%EB%B2%A0%EB%94%94%EB%93%9C-AI-%EA%B7%B8%EB%A6%AC%EA%B3%A0-FMOps" target="_blank" rel="noopener noreferrer"><code>@euisuk-chung</code> / 2025년 AI 트렌드: LMM, LAM, 온디바이스 AI, AI 에이전트, 임베디드 AI, 그리고 FMOps</a></li><li><a href="https://velog.io/@euisuk-chung/%ED%8A%B8%EB%A0%8C%EB%93%9C-%ED%8A%B8%EB%A0%8C%EC%8A%A4%ED%8F%AC%EB%A8%B8-%EC%9D%B4%ED%9B%84%EC%9D%98-%EC%B0%A8%EC%84%B8%EB%8C%80-%EC%95%84%ED%82%A4%ED%85%8D%EC%B3%90-MoE-SSM-RetNet-V-JEPA" target="_blank" rel="noopener noreferrer"><code>@euisuk-chung</code> / [트렌드] 트렌스포머 이후의 차세대 아키텍쳐: MoE, SSM, RetNet, V-JEPA</a></li><li><a href="https://velog.io/@euisuk-chung/NLP-Dimension-Reduction-Methods" target="_blank" rel="noopener noreferrer"><code>@euisuk-chung</code> / [NLP] 5. 자연어 차원 축소(Dimension Reduction) 기법</a></li></ul>',1)),t(" END: velog.io "),e[96]||(e[96]=s('<hr><h2 id="substack" tabindex="-1"><a class="header-anchor" href="#substack"><span>Substack</span></a></h2><ul><li><a href="https://aiencoder.substack.com/p/graphrag-analysis-part-1-how-indexing" target="_blank" rel="noopener noreferrer"><code>aiencoder</code> / GraphRAG Analysis, Part 1: How Indexing Elevates Knowledge Graph Performance in RAG</a></li><li><a href="https://weightythoughts.com/p/consider-the-llama" target="_blank" rel="noopener noreferrer"><code>weightythoughts</code> / Consider the Llama</a></li></ul><hr><h2 id="brunch" tabindex="-1"><a class="header-anchor" href="#brunch"><span>Brunch</span></a></h2><ul><li><a href="https://brunch.co.kr/@@ZVA/702" target="_blank" rel="noopener noreferrer"><code>@ZVA</code> / 어텐션맵은 뭐고 트랜스포머는 또 뭐냐...LLM 입문</a></li><li><a href="https://brunch.co.kr/@@2YWz/114" target="_blank" rel="noopener noreferrer"><code>@2YWz</code> / Enterprise LLM 사용자 인터페이스</a></li><li><a href="https://brunch.co.kr/@@5jl5/127" target="_blank" rel="noopener noreferrer"><code>@5jl5</code> / [책소개] 금융 AI의 이해 - 신용 평가, 사기 탐지, 퀀트투자, 생성형 AI를 활용한 실전 금융 AI</a></li><li><a href="https://brunch.co.kr/@@ZVA/722" target="_blank" rel="noopener noreferrer"><code>@ZVA</code> / 자바의 아버지 제임스 고슬링, 생성AI를 생각한다</a></li><li><a href="https://brunch.co.kr/@@ZVA/737" target="_blank" rel="noopener noreferrer"><code>@ZVA</code> / 생성AI와 읽기의 종말 시대, 가르친다는 것에 대하여</a></li><li><a href="https://brunch.co.kr/@@5jl5/132" target="_blank" rel="noopener noreferrer"><code>@5jl5</code> / RAG기반 LLM서비스 전망</a></li><li><a href="https://brunch.co.kr/@@5jl5/133" target="_blank" rel="noopener noreferrer"><code>@5jl5</code> / [책소개] Transformers &amp; LLMs 그림책 - Super Study Guide: Transformers &amp; LLMs</a></li><li><a href="https://brunch.co.kr/@@ZVA/752" target="_blank" rel="noopener noreferrer"><code>@ZVA</code> / 벡터 임베딩 기반 RAG는 왜 실패하는가</a></li><li><a href="https://brunch.co.kr/@@2rV/198" target="_blank" rel="noopener noreferrer"><code>@2rV</code> / 누구나 참여할 수 있는 프로토타이핑 - AI 코드 제너레이터와 함께하는 새로운 개발 문화</a></li></ul>',6)),t(" END: brunch.co.kr "),e[97]||(e[97]=s('<hr><h2 id="inblog" tabindex="-1"><a class="header-anchor" href="#inblog"><span>inblog</span></a></h2><ul><li><a href="https://inblog.ai/graphwoody/GraphRAG-01" target="_blank" rel="noopener noreferrer"><code>graphwoody</code> / From RAG to GraphRAG , What is the GraphRAG and why i use it?</a></li></ul><hr><h2 id="replicate" tabindex="-1"><a class="header-anchor" href="#replicate"><span>Replicate</span></a></h2><ul><li><a href="https://replicate.com/blog/run-llama-locally" target="_blank" rel="noopener noreferrer">A comprehensive guide to running Llama 2 locally</a></li></ul><hr><h2 id="metaai" tabindex="-1"><a class="header-anchor" href="#metaai"><span>MetaAI</span></a></h2><ul><li><a href="https://ai.meta.com/blog/audiocraft-musicgen-audiogen-encodec-generative-ai-audio" target="_blank" rel="noopener noreferrer">Open sourcing AudioCraft: Generative AI for audio made simple and available to all</a></li><li><a href="https://ai.meta.com/blog/code-llama-large-language-model-coding" target="_blank" rel="noopener noreferrer">Introducing Code Llama, a state-of-the-art large language model for coding</a></li><li><a href="https://ai.meta.com/blog/self-supervised-learning-the-dark-matter-of-intelligence" target="_blank" rel="noopener noreferrer">Self-supervised learning: The dark matter of intelligence</a></li><li><a href="https://ai.meta.com/blog/audiobox-generating-audio-voice-natural-language-prompts" target="_blank" rel="noopener noreferrer">Audiobox: Generating audio from voice and natural language prompts</a></li></ul><hr><h2 id="google-1" tabindex="-1"><a class="header-anchor" href="#google-1"><span>Google</span></a></h2><h3 id="colab-1" tabindex="-1"><a class="header-anchor" href="#colab-1"><span>Colab</span></a></h3>',12)),r("ul",null,[e[78]||(e[78]=r("li",null,[r("a",{href:"https://colab.research.google.com/drive/1Nq28vk9_l0R-53T18HYfRbeGFJoZ_U8E",target:"_blank",rel:"noopener noreferrer"},"MANATEE(lm) : Market Analysis based on language model architectures")],-1)),r("li",null,[r("a",w,[a(h,{icon:"fa-brands fa-python"}),e[76]||(e[76]=r("code",null,"llm-pricing-cost-quality.ipynb",-1))])]),r("li",null,[r("a",_,[a(h,{icon:"fa-brands fa-python"}),e[77]||(e[77]=r("code",null,"cudf_pandas_stocks_demo.ipynb",-1))])])]),e[98]||(e[98]=s('<hr><h2 id="ollama" tabindex="-1"><a class="header-anchor" href="#ollama"><span>Ollama</span></a></h2><ul><li><a href="https://ollama.ai/blog/run-llama2-uncensored-locally" target="_blank" rel="noopener noreferrer">Run Llama 2 Uncensored Locally</a></li><li><a href="https://ollama.com/blog/llama-3-is-not-very-censored" target="_blank" rel="noopener noreferrer">Llama 3 is not very censored</a></li></ul><hr><h2 id="linkedin" tabindex="-1"><a class="header-anchor" href="#linkedin"><span>Linkedin</span></a></h2><ul><li><a href="https://www.linkedin.com/pulse/how-fit-large-language-models-small-memory-ivan-reznikov/" target="_blank" rel="noopener noreferrer">How to Fit Large Language Models in Small Memory: Quantization</a></li></ul><hr><h2 id="finbarr-timbers" tabindex="-1"><a class="header-anchor" href="#finbarr-timbers"><span>Finbarr Timbers</span></a></h2><ul><li><a href="https://finbarr.ca/how-is-llama-cpp-possible" target="_blank" rel="noopener noreferrer">How is LLaMa.cpp possible?</a></li></ul><hr><h2 id="fast-ai" tabindex="-1"><a class="header-anchor" href="#fast-ai"><span>fast.ai</span></a></h2><ul><li><a href="https://www.fast.ai/posts/2023-09-04-learning-jumps" target="_blank" rel="noopener noreferrer">Can LLMs learn from a single example?</a></li></ul><hr><h2 id="replit" tabindex="-1"><a class="header-anchor" href="#replit"><span>replit</span></a></h2><ul><li><a href="https://blog.replit.com/building-my-first-slack-bot" target="_blank" rel="noopener noreferrer">I&#39;m not a programmer, and I used AI to build my first bot</a></li></ul><hr><h2 id="second-state" tabindex="-1"><a class="header-anchor" href="#second-state"><span>Second State</span></a></h2><ul><li><a href="https://www.secondstate.io/articles/fast-llm-inference/" target="_blank" rel="noopener noreferrer">Fast and Portable Llama2 Inference on the Heterogeneous Edge</a></li></ul><hr><h2 id="bbycroft" tabindex="-1"><a class="header-anchor" href="#bbycroft"><span>Bbycroft</span></a></h2><ul><li><a href="https://bbycroft.net/llm" target="_blank" rel="noopener noreferrer">LLM Visualization</a></li></ul><hr><h2 id="justine" tabindex="-1"><a class="header-anchor" href="#justine"><span>Justine</span></a></h2>',23)),r("ul",null,[r("li",null,[e[80]||(e[80]=r("a",{href:"https://justine.lol/oneliners",target:"_blank",rel:"noopener noreferrer"},"Bash One-Liners for LLMs",-1)),r("ul",null,[r("li",null,[r("a",I,[a(h,{icon:"iconfont icon-shell"}),e[79]||(e[79]=r("code",null,"jart/rename-pictures.sh",-1))])])])])]),e[99]||(e[99]=s('<hr><h2 id="moomou" tabindex="-1"><a class="header-anchor" href="#moomou"><span>moomou</span></a></h2><ul><li><a href="https://paul.mou.dev/posts/2023-12-31-listening-with-llm/" target="_blank" rel="noopener noreferrer">Listening with LLM</a></li></ul><hr><h2 id="trail-of-bits" tabindex="-1"><a class="header-anchor" href="#trail-of-bits"><span>Trail of Bits</span></a></h2><ul><li><a href="https://blog.trailofbits.com/2024/01/16/leftoverlocals-listening-to-llm-responses-through-leaked-gpu-local-memory" target="_blank" rel="noopener noreferrer">LeftoverLocals: Listening to LLM responses through leaked GPU local memory</a></li></ul><hr><h2 id="_2mb-codes" tabindex="-1"><a class="header-anchor" href="#_2mb-codes"><span>2MB codes</span></a></h2><ul><li><a href="https://2mb.codes/~cmb/ollama-bot/" target="_blank" rel="noopener noreferrer"><code>cmb/ollama-bot</code>: This is a rudimentary IRC bot that communicates with a local instance of ollama.</a></li></ul><hr><h2 id="shyam-s-blog" tabindex="-1"><a class="header-anchor" href="#shyam-s-blog"><span>Shyam&#39;s Blog</span></a></h2><ul><li><a href="https://shyam.blog/posts/beyond-self-attention/" target="_blank" rel="noopener noreferrer">Beyond Self-Attention: How a Small Language Model Predicts the Next Token</a></li></ul><hr><h2 id="gradient-descent-into-madness" tabindex="-1"><a class="header-anchor" href="#gradient-descent-into-madness"><span>Gradient Descent into Madness</span></a></h2><ul><li><a href="https://bclarkson-code.github.io/posts/llm-from-scratch-scalar-autograd/post.html" target="_blank" rel="noopener noreferrer">LLM from scratch: Automatic Differentiation</a></li></ul><hr><h2 id="hamel" tabindex="-1"><a class="header-anchor" href="#hamel"><span>Hamel</span></a></h2><ul><li><a href="https://hamel.dev/blog/posts/prompt/" target="_blank" rel="noopener noreferrer">Fuck You, Show Me The Prompt.</a></li></ul><hr><h2 id="ahead-of-ai" tabindex="-1"><a class="header-anchor" href="#ahead-of-ai"><span>Ahead of AI</span></a></h2><ul><li><a href="https://sebastianraschka.com/blog/2023/optimizing-LLMs-dataset-perspective.html" target="_blank" rel="noopener noreferrer">Optimizing LLMs From a Dataset Perspective</a></li><li><a href="https://magazine.sebastianraschka.com/p/lora-and-dora-from-scratch" target="_blank" rel="noopener noreferrer">Improving LoRA: Implementing Weight-Decomposed Low-Rank Adaptation (DoRA) from Scratch</a></li></ul><hr><h2 id="kapa-ai" tabindex="-1"><a class="header-anchor" href="#kapa-ai"><span>kapa.ai</span></a></h2><ul><li><a href="https://docs.kapa.ai/blog/optimizing-technical-documentation-for-llms" target="_blank" rel="noopener noreferrer">Optimizing Technical Docs for LLMs</a></li></ul><hr><h2 id="cloudflare" tabindex="-1"><a class="header-anchor" href="#cloudflare"><span>Cloudflare</span></a></h2><ul><li><a href="https://blog.cloudflare.com/firewall-for-ai" target="_blank" rel="noopener noreferrer">Cloudflare announces Firewall for AI</a></li></ul><hr><h2 id="martin-lumiste" tabindex="-1"><a class="header-anchor" href="#martin-lumiste"><span>Martin Lumiste</span></a></h2><ul><li><a href="https://mlumiste.com/technical/compression-deep-learning/" target="_blank" rel="noopener noreferrer">Compressing images with neural networks</a></li></ul><hr><h2 id="brandon-s-digital-garden" tabindex="-1"><a class="header-anchor" href="#brandon-s-digital-garden"><span>Brandon&#39;s Digital Garden</span></a></h2><ul><li><a href="https://ngacho.com/blog/emaillm/" target="_blank" rel="noopener noreferrer">Building an email to calendar LLM</a></li></ul><hr><h2 id="justine-tunney-s-web-page" tabindex="-1"><a class="header-anchor" href="#justine-tunney-s-web-page"><span>Justine Tunney&#39;s Web Page</span></a></h2><ul><li><a href="https://justine.lol/matmul/" target="_blank" rel="noopener noreferrer">LLaMA Now Goes Faster on CPUs</a></li></ul><hr><h2 id="hiddenest" tabindex="-1"><a class="header-anchor" href="#hiddenest"><span>hiddenest</span></a></h2><ul><li><a href="https://hiddenest.dev/three-hours-of-llm" target="_blank" rel="noopener noreferrer">llm은 모르지만 뭔가 만들고 싶어서</a></li></ul><hr><h2 id="tistory" tabindex="-1"><a class="header-anchor" href="#tistory"><span>tistory</span></a></h2><ul><li><a href="https://iostream.tistory.com/m/" target="_blank" rel="noopener noreferrer"><code>iostream</code> / Make headway towards solving the problem</a><ul><li><a href="https://iostream.tistory.com/m/179" target="_blank" rel="noopener noreferrer">Full Stack Optimization of Transformer Inference: a Survey (1)</a></li></ul><!-- END: iostream --></li><li><a href="https://cori.tistory.com/m/" target="_blank" rel="noopener noreferrer"><code>cori</code> / 코딩하는 오리</a><ul><li><a href="https://cori.tistory.com/m/347" target="_blank" rel="noopener noreferrer">나의 개발 일지 (3) RAG 구현 및 개선</a></li></ul><!-- END: cori --></li><li><a href="https://soohey.tistory.com/m/" target="_blank" rel="noopener noreferrer"><code>soohey</code> / 개발 아카이빙</a><ul><li><a href="https://soohey.tistory.com/m/86" target="_blank" rel="noopener noreferrer">벤치마크 데이터셋에 대해</a></li><li><a href="https://soohey.tistory.com/m/87" target="_blank" rel="noopener noreferrer">실전 LLM chp1</a></li></ul><!-- END: soohey --></li><li><a href="https://csj000714.tistory.com/m/" target="_blank" rel="noopener noreferrer"><code>csj000714</code> / 드프 DrawingProcess</a><ul><li><a href="https://csj000714.tistory.com/m/1199" target="_blank" rel="noopener noreferrer">[Gen AI] 생성형 모델들의 원리 비교: VAE, GAN, Flow-based, Diffusion</a></li><li><a href="https://csj000714.tistory.com/m/1150" target="_blank" rel="noopener noreferrer">[Gen AI] 생성형 모델 및 서비스 정리</a></li><li><a href="https://csj000714.tistory.com/m/1156" target="_blank" rel="noopener noreferrer">[논문리뷰] LiDAR2Map: LiDAR-based distillation scheme - LiDAR2Map: In Defense of LiDAR-Based Semantic Map Construction Using Online Camera Distillation (CVPR 2023)</a></li><li><a href="https://csj000714.tistory.com/m/1201" target="_blank" rel="noopener noreferrer">[Gen AI] Generative Adversarial Network(GAN) 설명: 기초</a></li><li><a href="https://csj000714.tistory.com/m/1232" target="_blank" rel="noopener noreferrer">[논문 리뷰] HumanGaussian: Text-Driven 3D Human Generation with Gaussian Splatting (CVPR 2024)</a></li><li><a href="https://csj000714.tistory.com/m/1236" target="_blank" rel="noopener noreferrer">[논문 리뷰] HumanGaussian: Text-Driven 3D Human Generation with Gaussian Splatting (CVPR 2024)</a></li><li><a href="https://csj000714.tistory.com/m/1240" target="_blank" rel="noopener noreferrer">[논문리뷰] Spacetime Gaussian Feature Splatting for Real-Time Dynamic View Synthesis (CVPR 2024)</a></li><li><a href="https://csj000714.tistory.com/m/1244" target="_blank" rel="noopener noreferrer">[논문리뷰] DINO: Emerging Properties in Self-Supervised Vision Transformers (ICCV 2021)</a></li><li><a href="https://csj000714.tistory.com/m/1248" target="_blank" rel="noopener noreferrer">[논문 리뷰] D-NeRF: Neural Radiance Fields for Dynamic Scenes (CVPR 2021)</a></li><li><a href="https://csj000714.tistory.com/m/1229" target="_blank" rel="noopener noreferrer">[Career] CES 2025 사전 준비 및 관람</a></li><li><a href="https://csj000714.tistory.com/m/1251" target="_blank" rel="noopener noreferrer">[Project] 2024 경희대학교 SW페스티벌: 피지컬 컴퓨팅 분야(24.11.27.)</a></li><li><a href="https://csj000714.tistory.com/m/1253" target="_blank" rel="noopener noreferrer">[논문 리뷰] HyperNeRF : A Higher-Dimensional Representation for Topologically Varying Neural Radiance Fields (ACM TG 2021)</a></li><li><a href="https://csj000714.tistory.com/m/1254" target="_blank" rel="noopener noreferrer">[논문 리뷰] 4D Gaussian Splatting: 4D Gaussian Splatting for Real-Time Dynamic Scene Rendering (CVPR 2024)</a></li><li><a href="https://csj000714.tistory.com/m/1258" target="_blank" rel="noopener noreferrer">[논문 리뷰] Ha-NeRF: NeRFwithRealWorld + CNN Appearance Embedding - Hallucinated Neural Radiance Fields in the Wild</a></li></ul><!-- END: csj000714 --></li><li><a href="https://americanopeople.tistory.com/m/" target="_blank" rel="noopener noreferrer"><code>americanopeople</code> / 복세편살</a><ul><li><a href="https://americanopeople.tistory.com/m/462" target="_blank" rel="noopener noreferrer">NotebookLM: AI 시대에 원서 개발 서적 읽기</a></li></ul><!-- END: americanopeople --></li><li><a href="https://lsjsj92.tistory.com/m/" target="_blank" rel="noopener noreferrer"><code>lsjsj92</code> / 꿈 많은 사람의 이야기</a><ul><li><a href="https://lsjsj92.tistory.com/m/668" target="_blank" rel="noopener noreferrer">vLLM 사용법 - LLM을 쉽고 빠르게 추론(inference) 및 API 서빙(serving)하기</a></li><li><a href="https://lsjsj92.tistory.com/m/669" target="_blank" rel="noopener noreferrer">개인화를 고려한 LLM 모델 기반 추천 시스템 - PALR 추천 시스템 논문 리뷰</a></li><li><a href="https://lsjsj92.tistory.com/m/670" target="_blank" rel="noopener noreferrer">LLM과 추천 시스템을 결합해 설명가능성(Explainability) 제공하기(Feat. LangChain, GPT-4o)</a></li></ul><!-- END: lsjsj92 --></li><li><a href="http://bahnsville.tistory.com/m/" target="_blank" rel="noopener noreferrer"><code>bahnsville</code> / nthought</a><ul><li><a href="http://bahnsville.tistory.com/m/1264" target="_blank" rel="noopener noreferrer">모델 성능이 안 나올 때</a></li><li><a href="https://bahnsville.tistory.com/m/1265" target="_blank" rel="noopener noreferrer">LLM 왕국에서의 2년 Two Years in LLM</a></li></ul><!-- END: bahnsville --></li><li><a href="http://mobicon.tistory.com/m/" target="_blank" rel="noopener noreferrer"><code>mobicon</code> / AI Convergence</a><ul><li><a href="http://mobicon.tistory.com/m/605" target="_blank" rel="noopener noreferrer">[Embedding] 중간값과 백터</a></li><li><a href="http://mobicon.tistory.com/m/607" target="_blank" rel="noopener noreferrer">[LCC-1] LangChain Concept - Components &amp; RAG</a></li><li><a href="https://mobicon.tistory.com/m/614" target="_blank" rel="noopener noreferrer">[LCC-5] LangChain VectorStore 이해</a></li></ul><!-- END: mobicon --></li><li><a href="https://pearlluck.tistory.com/m/" target="_blank" rel="noopener noreferrer"><code>pearlluck</code> / 데엔잘하고싶은데엔🔥💎</a><ul><li><a href="https://pearlluck.tistory.com/m/821" target="_blank" rel="noopener noreferrer">Triton Inference Server 모델서빙1 - NVIDA Triton(트리톤)이란?</a></li><li><a href="https://pearlluck.tistory.com/m/824" target="_blank" rel="noopener noreferrer">Embedding을 저장하는 VectorDB 그리고 벡터 유사도 검색 Indexing</a></li></ul><!-- END: pearlluck --></li><li><a href="https://kesakiyo.tistory.com/m/" target="_blank" rel="noopener noreferrer"><code>kesakiyo</code> / 오늘도 개발로그</a><ul><li><a href="https://kesakiyo.tistory.com/m/entry/%EB%AF%B8%EB%93%9C%EC%A0%80%EB%8B%88%EC%97%90%EC%84%9C-%EC%9B%90%ED%95%98%EB%8A%94-%EC%9A%94%EC%86%8C-%EC%A0%9C%EA%B1%B0%ED%95%98%EA%B8%B0-no-%EC%98%B5%EC%85%98%EA%B3%BC-%EB%A9%80%ED%8B%B0-%ED%94%84%EB%A1%AC%ED%94%84%ED%8A%B8" target="_blank" rel="noopener noreferrer">미드저니에서 원하는 요소 제거하기: <code>--no</code> 옵션과 멀티 프롬프트</a></li><li><a href="https://kesakiyo.tistory.com/m/entry/%EB%AF%B8%EB%93%9C%EC%A0%80%EB%8B%88%EC%97%90%EC%84%9C-chaos-%EC%98%B5%EC%85%98-%EC%82%AC%EC%9A%A9%EB%B2%95-%EC%9D%B4%EB%AF%B8%EC%A7%80-%EB%8B%A4%EC%96%91%EC%84%B1-%EC%A1%B0%EC%A0%95-%ED%8C%81" target="_blank" rel="noopener noreferrer">미드저니에서 <code>--chaos</code>(<code>--c</code>) 옵션 사용법: 이미지 다양성 조정 팁</a></li><li><a href="https://kesakiyo.tistory.com/m/entry/%EB%AF%B8%EB%93%9C%EC%A0%80%EB%8B%88%EC%97%90%EC%84%9C-%EC%B0%BD%EC%9D%98%EC%A0%81%EC%9D%B8-%EC%9D%B4%EB%AF%B8%EC%A7%80%EB%A5%BC-%EC%83%9D%EC%84%B1%ED%95%98%EB%8A%94-%EB%B0%A9%EB%B2%95-weird-%EC%98%B5%EC%85%98" target="_blank" rel="noopener noreferrer">미드저니에서 창의적인 이미지를 생성하는 방법: <code>--weird</code>(<code>--w</code>) 옵션</a></li></ul><!-- END: kesakiyo --></li><li><a href="https://daeson.tistory.com/m/" target="_blank" rel="noopener noreferrer"><code>daeson</code> / 대소니</a><ul><li><a href="https://daeson.tistory.com/m/396" target="_blank" rel="noopener noreferrer">Retrieval Augmented Generation (RAG) and Beyond</a></li></ul><!-- END: daeson --></li><li><a href="https://hl1itj.tistory.com/m/" target="_blank" rel="noopener noreferrer"><code>hl1itj</code> / 쉽게 살 수 있을까 ?</a><ul><li><a href="https://hl1itj.tistory.com/m/268" target="_blank" rel="noopener noreferrer">생성형 AI 시대의 취업 준비</a></li></ul><!-- END: hl1itj --></li><li><a href="https://syaku.tistory.com/m/" target="_blank" rel="noopener noreferrer"><code>syaku</code> / 샤쿠 블로그</a><ul><li><a href="https://syaku.tistory.com/m/412" target="_blank" rel="noopener noreferrer">2024년 최신 AI 코딩 도구 완벽 가이드: GitHub Copilot부터 Cursor AI까지 8가지 비교 분석</a></li><li><a href="https://syaku.tistory.com/m/413" target="_blank" rel="noopener noreferrer">2024 Comprehensive Guide to AI Coding Tools: Comparing 8 Tools from GitHub Copilot to Cursor AI</a></li></ul><!-- END: syaku --></li><li><a href="https://ravenkim97.tistory.com/m/" target="_blank" rel="noopener noreferrer"><code>ravenkim97</code> / Life Log</a><ul><li><a href="https://ravenkim97.tistory.com/m/511" target="_blank" rel="noopener noreferrer">llm 활용 사례</a></li></ul><!-- END: ravenkim97 --></li><li><a href="https://jeongchul.tistory.com/m/" target="_blank" rel="noopener noreferrer"><code>jeongchul</code> / Jeongchul Kim</a><ul><li><a href="https://jeongchul.tistory.com/m/839" target="_blank" rel="noopener noreferrer">ML Interview - Transfer Learning</a></li><li><a href="https://jeongchul.tistory.com/m/840" target="_blank" rel="noopener noreferrer">ML Interview - Anomaly Detection</a></li><li><a href="https://jeongchul.tistory.com/m/841" target="_blank" rel="noopener noreferrer">ML Interview - Z Score</a></li><li><a href="https://jeongchul.tistory.com/m/842" target="_blank" rel="noopener noreferrer">ML Interview - IQR</a></li><li><a href="https://jeongchul.tistory.com/m/843" target="_blank" rel="noopener noreferrer">ML Interview - 하이퍼파라미터 튜닝 기법</a></li><li><a href="https://jeongchul.tistory.com/m/844" target="_blank" rel="noopener noreferrer">ML Interview - Bayesian Optimization</a></li><li><a href="https://jeongchul.tistory.com/m/845" target="_blank" rel="noopener noreferrer">ML Interview - Evolutionary Algorithms</a></li><li><a href="https://jeongchul.tistory.com/m/846" target="_blank" rel="noopener noreferrer">ML Interview - Normalization</a></li><li><a href="https://jeongchul.tistory.com/m/847" target="_blank" rel="noopener noreferrer">ML Interview - 추천 시스템의 고수준 설계</a></li><li><a href="https://jeongchul.tistory.com/m/848" target="_blank" rel="noopener noreferrer">ML Interview - 모델 추론 실시간 서빙 시스템</a></li><li><a href="https://jeongchul.tistory.com/m/849" target="_blank" rel="noopener noreferrer">ML Interview - Cross Validation</a></li><li><a href="https://jeongchul.tistory.com/m/850" target="_blank" rel="noopener noreferrer">ML Interview - 딥러닝의 장점과 단점</a></li><li><a href="https://jeongchul.tistory.com/m/851" target="_blank" rel="noopener noreferrer">ML Interview - Curse of Dimensionality 차원의 저주</a></li><li><a href="https://jeongchul.tistory.com/m/852" target="_blank" rel="noopener noreferrer">ML Interview - Parallelism with NVLink</a></li></ul><!-- END: jeongchul --></li><li><a href="https://aspdotnet.tistory.com/m/" target="_blank" rel="noopener noreferrer"><code>aspdotnet</code> / 재우니의 블로그</a><ul><li><a href="https://aspdotnet.tistory.com/m/3330" target="_blank" rel="noopener noreferrer">2024년 최신 AI 개발 도구 비교: Replit, Bolt.new, Cursor AI</a></li></ul><!-- END: aspdotnet --></li><li><a href="https://shplab.tistory.com/m/" target="_blank" rel="noopener noreferrer"><code>shplab</code> / 박서희연구소</a><ul><li><a href="https://shplab.tistory.com/m/entry/Model-Transformer-Model%ED%8A%B8%EB%9E%9C%EC%8A%A4%ED%8F%AC%EB%A8%B8-%EB%AA%A8%EB%8D%B8" target="_blank" rel="noopener noreferrer">[Model] Transformer Model(트랜스포머 모델)</a></li><li><a href="https://shplab.tistory.com/m/entry/Model-Transformer-Model-Pipeline%ED%8A%B8%EB%9E%9C%EC%8A%A4%ED%8F%AC%EB%A8%B8-%EB%AA%A8%EB%8D%B8-%ED%8C%8C%EC%9D%B4%ED%94%84%EB%9D%BC%EC%9D%B8" target="_blank" rel="noopener noreferrer">[Model] Transformer Model Pipeline(트랜스포머 모델 파이프라인)</a></li></ul><!-- END: shplab --></li></ul>',42)),t(" END: tistory.com "),e[100]||(e[100]=r("hr",null,null,-1)),e[101]||(e[101]=r("h2",{id:"real-python",tabindex:"-1"},[r("a",{class:"header-anchor",href:"#real-python"},[r("span",null,"Real Python")])],-1)),e[102]||(e[102]=r("ul",null,[r("li",null,[r("a",{href:"https://realpython.com/huggingface-transformers/",target:"_blank",rel:"noopener noreferrer"},"Hugging Face Transformers: Leverage Open-Source AI in Python")]),r("li",null,[r("a",{href:"https://realpython.com/nltk-nlp-python/",target:"_blank",rel:"noopener noreferrer"},"Natural Language Processing (NLP) with Python's Natural Language Toolkit (NLTK).")])],-1)),t(" END: realpython.com "),e[103]||(e[103]=s('<hr><h2 id="정우일-블로그" tabindex="-1"><a class="header-anchor" href="#정우일-블로그"><span>정우일 블로그</span></a></h2><ul><li><a href="https://wooiljeong.github.io/ml/gguf-llm/" target="_blank" rel="noopener noreferrer">GGUF 파일로 로컬에서 LLM 실행하기</a></li></ul><hr><h2 id="daddy-maker" tabindex="-1"><a class="header-anchor" href="#daddy-maker"><span>Daddy Maker</span></a></h2><ul><li><a href="https://daddynkidsmakers.blogspot.com/2024/05/llamaindex-llm.html" target="_blank" rel="noopener noreferrer">LlamaIndex와 LLM 기반 유해, 무해 이미지 인식 기술 개발 방법</a></li><li><a href="https://daddynkidsmakers.blogspot.com/2024/05/rag.html" target="_blank" rel="noopener noreferrer">LLM 기반 그래프 RAG 기술 구현하기</a></li><li><a href="https://daddynkidsmakers.blogspot.com/2024/06/llm-rag-chroma.html" target="_blank" rel="noopener noreferrer">대형언어모델 검색증강생성의 핵심기술, 벡터 데이터베이스 Chroma 분석하기</a></li><li><a href="https://daddynkidsmakers.blogspot.com/2024/06/llm.html" target="_blank" rel="noopener noreferrer">다중 에이전트 LLM 아키텍처 소개</a></li><li><a href="https://daddynkidsmakers.blogspot.com/2024/06/llm_16.html" target="_blank" rel="noopener noreferrer">LLM 의 통계적 패턴 예측성과 한계에 대한 연구</a></li><li><a href="https://daddynkidsmakers.blogspot.com/2024/06/nlp.html" target="_blank" rel="noopener noreferrer">NLP의 핵심. 토큰, 임베딩과 파인튜닝</a></li><li><a href="https://daddynkidsmakers.blogspot.com/2024/06/lora-llama3.html" target="_blank" rel="noopener noreferrer">도메인 모델 성능개선을 위한 Lora 기반 LLAMA3 모델 파인튜닝하기</a></li><li><a href="https://daddynkidsmakers.blogspot.com/2024/06/weights-biases.html" target="_blank" rel="noopener noreferrer">Weights &amp; Biases로 딥러닝 모델 개발 프로세스 기록, 분석, 가시화 및 모델 튜닝하기</a></li><li><a href="https://daddynkidsmakers.blogspot.com/2024/06/2024.html" target="_blank" rel="noopener noreferrer">2024년 오픈소스 대형언어모델 소개</a></li><li><a href="https://daddynkidsmakers.blogspot.com/2024/07/autorag-llm-rag.html" target="_blank" rel="noopener noreferrer">AutoRAG 활용 LLM RAG 최적화하기</a></li><li><a href="https://daddynkidsmakers.blogspot.com/2024/07/llm_17.html" target="_blank" rel="noopener noreferrer">LLM 기반 센서 데이터 해석 방법</a></li><li><a href="https://daddynkidsmakers.blogspot.com/2024/07/sllm-vllm.html" target="_blank" rel="noopener noreferrer">sLLM과 vLLM에 대한 이야기</a></li><li><a href="https://daddynkidsmakers.blogspot.com/2024/08/ai-sakanaai.html" target="_blank" rel="noopener noreferrer">AI 과학자 Sakana.AI 사용법</a></li><li><a href="https://daddynkidsmakers.blogspot.com/2024/08/blog-post.html" target="_blank" rel="noopener noreferrer">인공지능 딥러닝 모델 성능 지표</a></li><li><a href="https://daddynkidsmakers.blogspot.com/2024/09/blog-post.html" target="_blank" rel="noopener noreferrer">효율적인 딥러닝 모델링을 위한 시계열 데이터 처리</a></li><li><a href="https://daddynkidsmakers.blogspot.com/2024/09/llm-rag-enhanced-visual-question.html" target="_blank" rel="noopener noreferrer">로컬 멀티모달 LLM 기반 간단한 RAG Enhanced Visual Question Answering 개발하기</a></li></ul>',6)),t(" END: daddynkidsmakers.blogspot.com "),e[104]||(e[104]=s('<hr><h2 id="outerbounds" tabindex="-1"><a class="header-anchor" href="#outerbounds"><span>Outerbounds</span></a></h2><ul><li><a href="https://outerbounds.com/blog/the-many-ways-to-deploy-a-model/" target="_blank" rel="noopener noreferrer">The Many Ways to Deploy a Model</a></li></ul><hr><h2 id="christophergs" tabindex="-1"><a class="header-anchor" href="#christophergs"><span>ChristopherGS</span></a></h2><ul><li><a href="https://christophergs.com/blog/running-open-source-llms-in-python" target="_blank" rel="noopener noreferrer">Running Open Source LLMs In Python - A Practical Guide</a></li></ul><hr><h2 id="simon-willison-s-tils" tabindex="-1"><a class="header-anchor" href="#simon-willison-s-tils"><span>Simon Willison&#39;s TILs</span></a></h2><ul><li><a href="https://simonwillison.net/2024/Jun/17/cli-language-models/" target="_blank" rel="noopener noreferrer">Language models on the command-line</a></li><li><a href="https://simonwillison.net/2024/May/29/training-not-chatting/" target="_blank" rel="noopener noreferrer">Training is not the same as chatting: ChatGPT and other LLMs don’t remember everything you say</a></li><li><a href="https://simonwillison.net/2024/Sep/29/notebooklm-audio-overview/" target="_blank" rel="noopener noreferrer">NotebookLM’s automatically generated podcasts are surprisingly effective</a></li><li><a href="https://simonwillison.net/2024/Oct/1/whisper-large-v3-turbo-model/" target="_blank" rel="noopener noreferrer">Whisper large-v3-turbo model.</a></li><li><a href="https://simonwillison.net/2024/Oct/17/video-scraping/" target="_blank" rel="noopener noreferrer">Video scraping: extracting JSON data from a 35 second screen capture for less than 1/10th of a cent</a></li></ul>',9)),t(" END: simonwillison.net "),e[105]||(e[105]=s('<hr><h2 id="allen-pike" tabindex="-1"><a class="header-anchor" href="#allen-pike"><span>Allen Pike</span></a></h2><ul><li><a href="https://allenpike.com/2024/llms-trained-on-internet" target="_blank" rel="noopener noreferrer">LLMs Aren’t Just “Trained On the Internet” Anymore</a></li></ul><hr><h2 id="냉동코더의-기술블로그" tabindex="-1"><a class="header-anchor" href="#냉동코더의-기술블로그"><span>냉동코더의 기술블로그</span></a></h2><ul><li><a href="https://cliearl.github.io/posts/etc/ollama-intro/" target="_blank" rel="noopener noreferrer">macOS에서 Ollama 사용하기</a></li></ul><hr><h2 id="applied-llms" tabindex="-1"><a class="header-anchor" href="#applied-llms"><span>Applied LLMs</span></a></h2><ul><li><a href="https://applied-llms.org/" target="_blank" rel="noopener noreferrer">What We’ve Learned From A Year of Building with LLMs</a></li></ul><hr><h2 id="oranlooney-com" tabindex="-1"><a class="header-anchor" href="#oranlooney-com"><span>OranLooney.com</span></a></h2><p><a href="https://www.oranlooney.com/post/gpt-cnn/" target="_blank" rel="noopener noreferrer">A Picture is Worth 170 Tokens: How Does GPT-4o Encode Images?</a></p><hr><h2 id="lytix-ai" tabindex="-1"><a class="header-anchor" href="#lytix-ai"><span>lytix.ai</span></a></h2><ul><li><a href="https://blog.lytix.co/posts/self-hosting-llama-3" target="_blank" rel="noopener noreferrer">Cost Of Self Hosting Llama-3 8B-Instruct</a></li></ul><hr><h2 id="llama-ttf" tabindex="-1"><a class="header-anchor" href="#llama-ttf"><span><code>llama.ttf</code></span></a></h2><ul><li><a href="https://fuglede.github.io/llama.ttf/" target="_blank" rel="noopener noreferrer">llama.ttf</a></li></ul><hr><h2 id="eugene-yan" tabindex="-1"><a class="header-anchor" href="#eugene-yan"><span>Eugene Yan</span></a></h2><ul><li><a href="https://eugeneyan.com/writing/llm-patterns/" target="_blank" rel="noopener noreferrer">Patterns for Building LLM-based Systems &amp; Products</a></li></ul><hr><h2 id="alex-strick-van-linschoten" tabindex="-1"><a class="header-anchor" href="#alex-strick-van-linschoten"><span>Alex Strick van Linschoten</span></a></h2><ul><li><a href="https://mlops.systems/posts/2024-06-25-evaluation-finetuning-manual-dataset.html" target="_blank" rel="noopener noreferrer">How to think about creating a dataset for LLM finetuning evaluation</a></li></ul><hr><h2 id="imbue" tabindex="-1"><a class="header-anchor" href="#imbue"><span>imbue</span></a></h2><ul><li><a href="https://imbue.com/research/70b-infrastructure/" target="_blank" rel="noopener noreferrer">From bare metal to a 70B model: infrastructure set-up and scripts</a></li></ul><hr><h2 id="jay-alammar" tabindex="-1"><a class="header-anchor" href="#jay-alammar"><span>Jay Alammar</span></a></h2><ul><li><a href="https://jalammar.github.io/illustrated-transformer/" target="_blank" rel="noopener noreferrer">The Illustrated Transformer</a></li></ul><hr><h2 id="the-missing-notes" tabindex="-1"><a class="header-anchor" href="#the-missing-notes"><span>The Missing Notes</span></a></h2><ul><li><a href="https://likejazz.com/post/755660739094724609" target="_blank" rel="noopener noreferrer">42dot LLM은 리더보드 용도로 파인튜닝한게 아니기 때문에 점수는 낮지만, 실제로 써보면 성능은 매우 뛰어난 걸 확인할 수 있습니다. 320장의 GPU로 약 1주일 동안</a></li><li><a href="https://likejazz.com/post/756113715506659328" target="_blank" rel="noopener noreferrer">현대자동차그룹의 후원으로 MIT 윤킴 교수님과 지난 1년여간 함께 진행해온 산학 연구 논문이 요즘 화제네요.</a></li><li><a href="https://likejazz.com/post/756204304905469952" target="_blank" rel="noopener noreferrer"><code>llama.cpp</code>의 K-Quantization https://github.com/ggerganov/llama.cpp/pull/1684 을 따라서 구현해보다가 길을 잃고 😑 좀 더</a></li><li><a href="https://likejazz.com/post/756294915544219648" target="_blank" rel="noopener noreferrer">The Era of 1-bit LLMs: All Large Language Models are in 1.58</a></li><li><a href="https://likejazz.com/post/756657294178140160" target="_blank" rel="noopener noreferrer">간밤에 메타에서 드디어 라마3를 공개했습니다. 역시나 기대했던 대로 압도적인 스케일과 성능을 자랑하네요. 2만 4천 장의 GPU, 15T 학습 데이터, 1천만 건의 인스트럭션</a></li><li><a href="http://likejazz.com/post/757019683183607808" target="_blank" rel="noopener noreferrer">곧 출시되는 ollama의 신규 버전에는 드디어 CJK 문제가 해결되면서 CLI에서 문제 없이 한글 처리가 될 것으로 보이네요. 이외에도 <code>llama.cpp</code>에 flash</a></li><li><a href="http://likejazz.com/post/757200879566946304" target="_blank" rel="noopener noreferrer"><code>llm.c</code> 프로젝트로 요즘 pure C/CUDA training 코드를 만들고 있는 카파시가 이번에 GPT-2 124M 모델 학습을 1장의 GPU로 90분만에 재현했네요.</a></li></ul><hr><h2 id="포티투닷-42dot-we-are-a-mobility-ai-company" tabindex="-1"><a class="header-anchor" href="#포티투닷-42dot-we-are-a-mobility-ai-company"><span>포티투닷 | 42dot - We Are A Mobility AI Company</span></a></h2><ul><li><a href="https://42dot.ai/blog/178" target="_blank" rel="noopener noreferrer">42dot LLM 1.3B</a></li></ul><hr><h2 id="haandol-tl-dr" tabindex="-1"><a class="header-anchor" href="#haandol-tl-dr"><span>Haandol, TL;DR</span></a></h2><ul><li><a href="https://haandol.github.io/2024/07/27/demystifying-small-language-model-fine-tuning.html" target="_blank" rel="noopener noreferrer">SLM 파인튜닝 하기 전에 알아두면 좋은 내용 - 1/2</a></li><li><a href="https://haandol.github.io/2024/08/24/architecting-an-agentic-system.html" target="_blank" rel="noopener noreferrer">AI 에이전트 시스템을 설계할 때 알아두면 좋은 내용</a></li></ul><hr><h2 id="비즈니스-테크놀로지-리더십-cio-korea" tabindex="-1"><a class="header-anchor" href="#비즈니스-테크놀로지-리더십-cio-korea"><span>비즈니스, 테크놀로지, 리더십 - CIO Korea</span></a></h2><ul><li><a href="https://ciokorea.com/news/345545" target="_blank" rel="noopener noreferrer">엔비디아, 새로운 네모 리트리버 마이크로서비스 발표··· &quot;LLM 정확도 및 처리량 향상 지원&quot;</a></li><li><a href="https://ciokorea.com/news/345559" target="_blank" rel="noopener noreferrer">상용 RAG의 현주소와 도전 과제</a></li><li><a href="https://ciokorea.com/news/344684" target="_blank" rel="noopener noreferrer">“로컬 기기에서 AI 성능 극대화”··· 허깅페이스, 소형 언어 모델 ‘스몰LM’ 공개</a></li><li><a href="https://ciokorea.com/news/351362" target="_blank" rel="noopener noreferrer">카카오, AI 언어모델 성능 평가 데이터셋 구축 및 오픈소스 공개</a></li><li><a href="https://ciokorea.com/news/351306" target="_blank" rel="noopener noreferrer">이참에 알아둘까··· 주요 생성형 AI 용어 23가지</a></li></ul>',42)),t(" END: ciokorea.com "),e[106]||(e[106]=s('<hr><h2 id="테크놀로지-리더를-위한-글로벌-it-뉴스-itworld-korea" tabindex="-1"><a class="header-anchor" href="#테크놀로지-리더를-위한-글로벌-it-뉴스-itworld-korea"><span>테크놀로지 리더를 위한 글로벌 IT 뉴스 - ITWorld Korea</span></a></h2><ul><li><a href="https://itworld.co.kr/news/345524" target="_blank" rel="noopener noreferrer">“LLM 시장을 흔드는 메타 라마 3.1” 기업과 솔루션 업체의 득실</a></li><li><a href="https://itworld.co.kr/news/345482" target="_blank" rel="noopener noreferrer">RAG 산업화 여정의 현재와 미래</a></li><li><a href="https://itworld.co.kr/news/345734" target="_blank" rel="noopener noreferrer">&quot;모델에 구애받지 않는 LLM 프롬프트 관리&quot; 프롬프티 시작 가이드</a></li><li><a href="https://itworld.co.kr/news/346616" target="_blank" rel="noopener noreferrer">AI 환상을 현실로 옮기는 가장 효과적인 방법</a></li><li><a href="https://itworld.co.kr/news/346070" target="_blank" rel="noopener noreferrer">&quot;프로젝트 3개 중 1개는 폐기&quot; 생성형 AI 가치 입증이 어려운 이유</a></li><li><a href="https://itworld.co.kr/news/346882" target="_blank" rel="noopener noreferrer">커뮤니티에 공개된 애플 인텔리전스 &#39;비밀 지침&#39;으로 보는 생성형 AI의 과제</a></li><li><a href="https://itworld.co.kr/news/351586" target="_blank" rel="noopener noreferrer">LLM 프로젝트를 프로덕션에 도입하기 위한 5가지 과제와 해결책</a></li><li><a href="https://itworld.co.kr/news/350963" target="_blank" rel="noopener noreferrer">LLM이 아니라 애플리케이션이 필요한 이유</a></li></ul>',3)),t(" END: itworld.co.kr "),e[107]||(e[107]=s('<hr><h2 id="speaker-deck-easily-share-your-presentations-online" tabindex="-1"><a class="header-anchor" href="#speaker-deck-easily-share-your-presentations-online"><span>Speaker Deck | Easily Share Your Presentations Online</span></a></h2><ul><li><a href="https://speakerdeck.com/huffon/what-if-dot-dot-dot-ceoeumbuteo-dasi-llm-eopeulrikeisyeoneul-gaebalhandamyeon" target="_blank" rel="noopener noreferrer"><code>huffon</code> / What if...? 처음부터 다시 LLM 어플리케이션을 개발한다면</a></li></ul><hr><h2 id="dable-tech-blog" tabindex="-1"><a class="header-anchor" href="#dable-tech-blog"><span>Dable Tech Blog</span></a></h2><ul><li><a href="https://teamdable.github.io/techblog/Boosting-Fine-Tuning-of-LM" target="_blank" rel="noopener noreferrer">언어 모델의 Fine-Tuning 성능 올리기</a></li></ul><hr><h2 id="augmend" tabindex="-1"><a class="header-anchor" href="#augmend"><span>Augmend</span></a></h2><ul><li><a href="https://augmend.com/blog/TreeSeg" target="_blank" rel="noopener noreferrer">TreeSeg: Hierarchical Topic Segmentation at Augmend</a></li></ul><hr><h2 id="min-hsu" tabindex="-1"><a class="header-anchor" href="#min-hsu"><span>Min Hsu</span></a></h2><ul><li><a href="https://myhsu.xyz/llvm-sched-model-1/" target="_blank" rel="noopener noreferrer">Scheduling Model in LLVM - Part I</a></li></ul><hr><h2 id="chip-huyen" tabindex="-1"><a class="header-anchor" href="#chip-huyen"><span>Chip Huyen</span></a></h2><ul><li><a href="https://huyenchip.com/2024/07/25/genai-platform.html" target="_blank" rel="noopener noreferrer">Building A Generative AI Platform</a></li></ul><hr><h2 id="codesolvent-blog" tabindex="-1"><a class="header-anchor" href="#codesolvent-blog"><span>Codesolvent Blog</span></a></h2><ul><li><a href="https://blog.codesolvent.com/2024/09/declarative-programming-with-aillms.html" target="_blank" rel="noopener noreferrer">Declarative Programming With AI/LLMs</a></li></ul><hr><h2 id="최윤섭의-디지털-헬스케어" tabindex="-1"><a class="header-anchor" href="#최윤섭의-디지털-헬스케어"><span>최윤섭의 디지털 헬스케어</span></a></h2><ul><li><a href="https://yoonsupchoi.com/2024/10/03/how-to-regulate-generative-ai-in-healthcare/" target="_blank" rel="noopener noreferrer">생성형 의료 인공지능을 ‘새로운 지적 존재’로서 규제하자</a></li></ul>',21)),t(" END: yoonsupchoi.com "),e[108]||(e[108]=r("hr",null,null,-1)),e[109]||(e[109]=r("h2",{id:"timescale-blog",tabindex:"-1"},[r("a",{class:"header-anchor",href:"#timescale-blog"},[r("span",null,"Timescale Blog")])],-1)),e[110]||(e[110]=r("ul",null,[r("li",null,[r("a",{href:"https://timescale.com/blog/rag-is-more-than-just-vector-search/",target:"_blank",rel:"noopener noreferrer"},"RAG Is More Than Just Vector Search")])],-1)),t(" END: timescale.com "),e[111]||(e[111]=s('<hr><h2 id="_000namc-blog" tabindex="-1"><a class="header-anchor" href="#_000namc-blog"><span>000namc.blog</span></a></h2><ul><li><a href="https://000namc.github.io/2024/10/18/alexnet/" target="_blank" rel="noopener noreferrer">AlexNet 논문 리뷰</a></li><li><a href="https://000namc.github.io/2024/10/21/vgg/" target="_blank" rel="noopener noreferrer">VGG 논문 리뷰</a></li><li><a href="https://000namc.github.io/2024/10/23/resnet/" target="_blank" rel="noopener noreferrer">ResNet 논문 리뷰</a></li><li><a href="https://000namc.github.io/2024/10/25/nlp-models/" target="_blank" rel="noopener noreferrer">NLP 분야 논문리뷰 및 구현</a></li></ul>',3)),t(" END: 000namc.github.io "),e[112]||(e[112]=r("hr",null,null,-1)),e[113]||(e[113]=r("h2",{id:"sequoia-capital",tabindex:"-1"},[r("a",{class:"header-anchor",href:"#sequoia-capital"},[r("span",null,"Sequoia Capital")])],-1)),e[114]||(e[114]=r("ul",null,[r("li",null,[r("a",{href:"https://sequoiacap.com/article/generative-ais-act-o1/",target:"_blank",rel:"noopener noreferrer"},"Generative AI’s Act o1 - 에이전트 추론의 시대 개막")])],-1)),t(" END: sequoiacap.com "),e[115]||(e[115]=r("hr",null,null,-1)),e[116]||(e[116]=r("h2",{id:"_000namc-xyz",tabindex:"-1"},[r("a",{class:"header-anchor",href:"#_000namc-xyz"},[r("span",null,"000namc.xyz")])],-1)),e[117]||(e[117]=r("ul",null,[r("li",null,[r("a",{href:"https://blog.000namc.xyz/2024/10/29/nlp-models/",target:"_blank",rel:"noopener noreferrer"},"NLP 분야 논문리뷰 및 구현")]),r("li",null,[r("a",{href:"https://blog.000namc.xyz/2024/10/31/transformer/",target:"_blank",rel:"noopener noreferrer"},"(Transformer) Attention Is All You Need 리뷰")])],-1)),t(" END: blog.000namc.xyz "),e[118]||(e[118]=s('<hr><h2 id="tensormsa" tabindex="-1"><a class="header-anchor" href="#tensormsa"><span>TensorMSA</span></a></h2><ul><li><a href="https://hugrypiggykim.com/2024/11/02/recent-rag-research-survey/" target="_blank" rel="noopener noreferrer">Recent RAG research survey</a></li><li><a href="https://hugrypiggykim.com/2024/11/02/mamba/" target="_blank" rel="noopener noreferrer">MAMBA</a></li><li><a href="https://hugrypiggykim.com/2024/11/02/llm-summary/" target="_blank" rel="noopener noreferrer">LLM SUMMARY</a></li></ul>',3)),t(" END: hugrypiggykim.com "),e[119]||(e[119]=r("hr",null,null,-1)),e[120]||(e[120]=r("h2",{id:"jason-kang",tabindex:"-1"},[r("a",{class:"header-anchor",href:"#jason-kang"},[r("span",null,"Jason Kang")])],-1)),e[121]||(e[121]=r("ul",null,[r("li",null,[r("a",{href:"https://jasonkang14.github.io/llm/what-is-direct-parameter-optimization",target:"_blank",rel:"noopener noreferrer"},"What Is Direct Parameter Optimization(DPO)?")])],-1)),t(" END: jasonkang14.github.io "),e[122]||(e[122]=r("hr",null,null,-1)),e[123]||(e[123]=r("h2",{id:"pega-devlog",tabindex:"-1"},[r("a",{class:"header-anchor",href:"#pega-devlog"},[r("span",null,"Pega Devlog")])],-1)),e[124]||(e[124]=r("ul",null,[r("li",null,[r("a",{href:"https://jehyunlee.github.io/2024/11/07/General-67_afore2024/",target:"_blank",rel:"noopener noreferrer"},"생성AI 활용 학회 발표 준비")]),r("li",null,[r("a",{href:"https://jehyunlee.github.io/2024/11/18/General-69_SNU/",target:"_blank",rel:"noopener noreferrer"},"인공지능을 활용한 슬기로운 연구생활")])],-1)),t(" END: jehyunlee.github.io "),e[125]||(e[125]=s('<hr><h2 id="dschloe" tabindex="-1"><a class="header-anchor" href="#dschloe"><span>DSChloe</span></a></h2><ul><li><a href="https://dschloe.github.io/settings/2025/googlecolab_huggingface_login/" target="_blank" rel="noopener noreferrer">HuggingFace Login on Google Colab</a></li><li><a href="https://dschloe.github.io/settings/2024/12/kaggle_submission_class_sample/" target="_blank" rel="noopener noreferrer">Kaggle ML Submission 클래스 만들기</a></li><li><a href="https://dschloe.github.io/settings/2024/12/nasdaq_get_data_sample/" target="_blank" rel="noopener noreferrer">Nasdaq Data Link를 활용한 데이터 수집</a></li></ul>',3)),t(" END: dschloe.github.io "),a(u)])}const R=f(c,[["render",j]]),T=JSON.parse('{"path":"/ai/llm/references.html","title":"References","lang":"ko-KR","frontmatter":{"lang":"ko-KR","title":"References","description":"LLM > References","icon":"fas fa-book-atlas","category":["LLM","References"],"tag":["ai","llm","llama"],"head":[["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"References\\",\\"image\\":[\\"\\"],\\"dateModified\\":null,\\"author\\":[]}"],["meta",{"property":"og:url","content":"https://chanhi2000.github.io/ai/llm/references.html"}],["meta",{"property":"og:site_name","content":"chanhi2000"}],["meta",{"property":"og:title","content":"References"}],["meta",{"property":"og:description","content":"LLM > References"}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"ko-KR"}],["meta",{"property":"article:tag","content":"llama"}],["meta",{"property":"article:tag","content":"llm"}],["meta",{"property":"article:tag","content":"ai"}],[{"meta":null},{"property":"og:title","content":"LLM > References"},{"property":"og:description","content":"References"},{"property":"og:url","content":"https://chanhi2000.github.io/ai/llm/references.html"}]]},"git":{},"readingTime":{"minutes":9.77,"words":2932},"filePathRelative":"ai/llm/references.md"}');export{R as comp,T as data};
