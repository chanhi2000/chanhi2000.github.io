import{_ as u}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as h,d as r,at as i,g as o,b as t,t as f,w as n,e as a,r as s,o as g}from"./app-6nZ4Bu4J.js";const c={},b={id:"frontmatter-title-관련",tabindex:"-1"},k={class:"header-anchor",href:"#frontmatter-title-관련"},y={class:"table-of-contents"},L={id:"ai-at-meta",tabindex:"-1"},M={class:"header-anchor",href:"#ai-at-meta"};function x(d,e){const m=s("VPIcon"),l=s("router-link"),p=s("TagLinks");return g(),h("div",null,[r("h1",b,[r("a",k,[r("span",null,f(d.$frontmatter.title)+" 관련",1)])]),r("nav",y,[r("ul",null,[r("li",null,[t(l,{to:"#ai-at-meta"},{default:n(()=>[t(m,{icon:"fa-brands fa-meta"}),e[0]||(e[0]=a("AI at Meta"))]),_:1}),r("ul",null,[r("li",null,[t(l,{to:"#blog"},{default:n(()=>e[1]||(e[1]=[a("Blog")])),_:1})]),r("li",null,[t(l,{to:"#research"},{default:n(()=>e[2]||(e[2]=[a("Research")])),_:1})])])]),r("li",null,[t(l,{to:"#brunch"},{default:n(()=>e[3]||(e[3]=[a("Brunch")])),_:1})]),r("li",null,[t(l,{to:"#tistory"},{default:n(()=>e[4]||(e[4]=[a("tistory")])),_:1})]),r("li",null,[t(l,{to:"#lmsys-org"},{default:n(()=>e[5]||(e[5]=[a("LMSYS Org")])),_:1})]),r("li",null,[t(l,{to:"#unsloth-ai-finetune-llama-3-mistral-llms"},{default:n(()=>e[6]||(e[6]=[a("Unsloth AI | Finetune Llama 3 & Mistral LLMs")])),_:1})]),r("li",null,[t(l,{to:"#the-missing-notes"},{default:n(()=>e[7]||(e[7]=[a("The Missing Notes")])),_:1})]),r("li",null,[t(l,{to:"#비즈니스-테크놀로지-리더십-cio-korea"},{default:n(()=>e[8]||(e[8]=[a("비즈니스, 테크놀로지, 리더십 - CIO Korea")])),_:1})]),r("li",null,[t(l,{to:"#daddy-maker"},{default:n(()=>e[9]||(e[9]=[a("Daddy Maker")])),_:1})]),r("li",null,[t(l,{to:"#felafax-blog"},{default:n(()=>e[10]||(e[10]=[a("Felafax Blog")])),_:1})]),r("li",null,[t(l,{to:"#another-blog-for-seen"},{default:n(()=>e[11]||(e[11]=[a("another blog for seen")])),_:1})])])]),e[13]||(e[13]=r("hr",null,null,-1)),r("h2",L,[r("a",M,[r("span",null,[t(m,{icon:"fa-brands fa-meta"}),e[12]||(e[12]=a("AI at Meta"))])])]),e[14]||(e[14]=i('<h3 id="blog" tabindex="-1"><a class="header-anchor" href="#blog"><span>Blog</span></a></h3><ul><li><a href="https://ai.meta.com/blog/meta-llama-3-1/" target="_blank" rel="noopener noreferrer">Introducing Llama 3.1: Our most capable models to date</a></li><li><a href="https://ai.meta.com/blog/segment-anything-2/" target="_blank" rel="noopener noreferrer">Introducing SAM 2: The next generation of Meta Segment Anything Model for videos and images</a></li><li><a href="https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/" target="_blank" rel="noopener noreferrer">Llama 3.2: Revolutionizing edge AI and vision with open, customizable models</a></li></ul>',2)),o(" END: ai.meta.com "),e[15]||(e[15]=i('<h3 id="research" tabindex="-1"><a class="header-anchor" href="#research"><span>Research</span></a></h3><ul><li><a href="https://ai.meta.com/research/publications/the-llama-3-herd-of-models/" target="_blank" rel="noopener noreferrer">The Llama 3 Herd of Models</a></li></ul><hr><h2 id="brunch" tabindex="-1"><a class="header-anchor" href="#brunch"><span>Brunch</span></a></h2><ul><li><a href="https://brunch.co.kr/@@2YWz/111" target="_blank" rel="noopener noreferrer"><code>@2YWz</code> / Mark Zuckerberg on Llama 3.1</a></li><li><a href="https://brunch.co.kr/@@2rV/153" target="_blank" rel="noopener noreferrer"><code>@2rV</code> / 오픈소스 Llama 3.1 주의사항 - Llama 3.1 커뮤니티 라이선스 약관 주의점</a></li></ul>',5)),o(" END: brunch.co.kr "),e[16]||(e[16]=i('<hr><h2 id="tistory" tabindex="-1"><a class="header-anchor" href="#tistory"><span>tistory</span></a></h2><ul><li><a href="https://skyil.tistory.com/m/" target="_blank" rel="noopener noreferrer"><code>skyil</code> / 컴퓨터와 수학, 몽상 조금</a><ul><li><a href="https://skyil.tistory.com/m/303" target="_blank" rel="noopener noreferrer">Llama 3.1 Vision Language Model 관련 요약 (Llama 3-V)</a></li></ul><!-- END: skyil --></li><li><a href="https://devs0n.tistory.com/m/" target="_blank" rel="noopener noreferrer"><code>devs0n</code> / Be an Overachiever</a><ul><li><a href="https://devs0n.tistory.com/m/196" target="_blank" rel="noopener noreferrer">CodeGPT로 IntelliJ에서 Ollama LLM 사용하기</a></li></ul><!-- END: devs0n --></li><li><a href="https://webnautes.tistory.com/m/" target="_blank" rel="noopener noreferrer"><code>webnautes</code> / 멈춤보단 천천히라도 </a><ul><li><a href="https://webnautes.tistory.com/m/2398" target="_blank" rel="noopener noreferrer">Llama 3.2 3B finetuning 해보기</a></li></ul><!-- END: webnautes --></li><li><a href="http://syaku.tistory.com/m/" target="_blank" rel="noopener noreferrer"><code>syaku</code> / 샤쿠 블로그</a><ul><li><a href="https://syaku.tistory.com/m/422" target="_blank" rel="noopener noreferrer">Visual Studio Code에서 llama 3.2 사용하기</a></li></ul><!-- END: syaku --></li></ul>',3)),o(" END: tistory.com "),e[17]||(e[17]=i('<h2 id="lmsys-org" tabindex="-1"><a class="header-anchor" href="#lmsys-org"><span>LMSYS Org</span></a></h2><ul><li><a href="https://lmsys.org/blog/2024-07-25-sglang-llama3/" target="_blank" rel="noopener noreferrer">Achieving Faster Open-Source Llama3 Serving with SGLang Runtime (vs. TensorRT-LLM, vLLM)</a></li></ul><hr><h2 id="unsloth-ai-finetune-llama-3-mistral-llms" tabindex="-1"><a class="header-anchor" href="#unsloth-ai-finetune-llama-3-mistral-llms"><span>Unsloth AI | Finetune Llama 3 &amp; Mistral LLMs</span></a></h2><ul><li><a href="https://unsloth.ai/blog/llama3-1" target="_blank" rel="noopener noreferrer">Finetune &amp; Run Llama 3.1 with Unsloth</a></li></ul><hr><h2 id="the-missing-notes" tabindex="-1"><a class="header-anchor" href="#the-missing-notes"><span>The Missing Notes</span></a></h2><ul><li><a href="https://likejazz.com/post/757291470304755712" target="_blank" rel="noopener noreferrer">지난번 라마3 모델의 순수 NumPy 구현에 이어 이번에는 라마3 모델을 순수 C/CUDA로 구현해봤습니다.</a></li><li><a href="https://likejazz.com/post/757653866022141952" target="_blank" rel="noopener noreferrer"><code>llama.cpp</code>의 내부 구현을 분석하다가 ggml을 이용한 간단한 matmul 샘플을 구현해 봤습니다(프로젝트 링크는 댓글에). 원래 llama.cpp도 Georgi</a></li></ul><hr><h2 id="비즈니스-테크놀로지-리더십-cio-korea" tabindex="-1"><a class="header-anchor" href="#비즈니스-테크놀로지-리더십-cio-korea"><span>비즈니스, 테크놀로지, 리더십 - CIO Korea</span></a></h2><ul><li><a href="https://ciokorea.com/news/346581" target="_blank" rel="noopener noreferrer">메타, 차세대 통합 모델 ‘SAM 2’ 출시 &quot;실시간으로 비디오·이미지에서 객체 분할&quot;</a></li></ul>',11)),o(" END: ciokorea.com "),e[18]||(e[18]=r("hr",null,null,-1)),e[19]||(e[19]=r("h2",{id:"daddy-maker",tabindex:"-1"},[r("a",{class:"header-anchor",href:"#daddy-maker"},[r("span",null,"Daddy Maker")])],-1)),e[20]||(e[20]=r("ul",null,[r("li",null,[r("a",{href:"https://daddynkidsmakers.blogspot.com/2024/09/webui-ollama.html",target:"_blank",rel:"noopener noreferrer"},"WebUI 기반 Ollama 서비스 구동 방법")])],-1)),o(" END: daddynkidsmakers.blogspot.com "),e[21]||(e[21]=r("hr",null,null,-1)),e[22]||(e[22]=r("h2",{id:"felafax-blog",tabindex:"-1"},[r("a",{class:"header-anchor",href:"#felafax-blog"},[r("span",null,"Felafax Blog")])],-1)),e[23]||(e[23]=r("ul",null,[r("li",null,[r("a",{href:"https://publish.obsidian.md/felafax/pages/Tune+Llama3+405B+on+AMD+MI300x+(our+journey)",target:"_blank",rel:"noopener noreferrer"},"Tune Llama3 405B on AMD MI300x (our journey)")])],-1)),o(" END: publish.obsidian.md/felafax "),e[24]||(e[24]=r("hr",null,null,-1)),e[25]||(e[25]=r("h2",{id:"another-blog-for-seen",tabindex:"-1"},[r("a",{class:"header-anchor",href:"#another-blog-for-seen"},[r("span",null,"another blog for seen")])],-1)),e[26]||(e[26]=r("ul",null,[r("li",null,[r("a",{href:"https://m.blog.naver.com/se2n/223443729640",target:"_blank",rel:"noopener noreferrer"},"라마(Llama) 3 계열 한국어 모델 블라썸 Bllossom 8B - 한국어 질의응답 파인튜닝 (feat. AWQ 양자화, vLLM 사용법)")])],-1)),o(" END: se2n (blog.naver.com) "),e[27]||(e[27]=r("hr",null,null,-1)),t(p)])}const R=u(c,[["render",x]]),D=JSON.parse('{"path":"/ai/mcp/references.html","title":"References","lang":"ko-KR","frontmatter":{"lang":"ko-KR","title":"References","description":"MCP > References","icon":"fas fa-book-atlas","category":["AI","LLM","MCP","References"],"tag":["ai","artificial-intelligence","llm","large-language-models","mcp","model-context-protocol"],"head":[["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"References\\",\\"image\\":[\\"\\"],\\"dateModified\\":null,\\"author\\":[]}"],["meta",{"property":"og:url","content":"https://chanhi2000.github.io/ai/mcp/references.html"}],["meta",{"property":"og:site_name","content":"chanhi2000"}],["meta",{"property":"og:title","content":"References"}],["meta",{"property":"og:description","content":"MCP > References"}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"ko-KR"}],["meta",{"property":"article:tag","content":"model-context-protocol"}],["meta",{"property":"article:tag","content":"mcp"}],["meta",{"property":"article:tag","content":"large-language-models"}],["meta",{"property":"article:tag","content":"llm"}],["meta",{"property":"article:tag","content":"artificial-intelligence"}],["meta",{"property":"article:tag","content":"ai"}],[{"meta":null},{"property":"og:title","content":"MCP > References"},{"property":"og:description","content":"References"},{"property":"og:url","content":"https://chanhi2000.github.io/ai/mcp/references.html"}]]},"git":{},"readingTime":{"minutes":1.07,"words":322},"filePathRelative":"ai/mcp/references.md"}');export{R as comp,D as data};
